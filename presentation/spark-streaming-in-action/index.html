<!DOCTYPE html>
<html>
  <head>
    <title>Spark Streaming in Action</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
 <style type="text/css">
@import url(https://fonts.googleapis.com/css?family=Droid+Serif);
@import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

body {
  font-family: 'Droid Serif';
}
h1, h2, h3 {
  font-family: 'Yanone Kaffeesatz';
  font-weight: 400;
  margin-bottom: 0;
}
.remark-slide-content h1 { font-size: 3em; }
.remark-slide-content h2 { font-size: 2em; }
.remark-slide-content h3 { font-size: 1.6em; }
.footnote {
position: absolute;
bottom: 3em;
}
li p { line-height: 1.25em; }
.red { color: #fa0000; }
.large { font-size: 2em; }
a, a > code {
color: rgb(249, 38, 114);
       text-decoration: none;
}
code {
  -moz-border-radius: 5px;
  -web-border-radius: 5px;
background: #e7e8e2;
            border-radius: 5px;
}
.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
.remark-code-line-highlighted     { background-color: #373832; }
.pull-left {
float: left;
width: 47%;
}
.pull-right {
float: right;
width: 47%;
}
.pull-right ~ p {
clear: both;
}
#slideshow .slide .content code {
  font-size: 0.8em;
}
#slideshow .slide .content pre code {
  font-size: 0.9em;
padding: 15px;
}
.inverse {
background: #272822;
color: #777872;
       text-shadow: 0 0 20px #333;
}
.inverse h1, .inverse h2 {
color: #f3f3f3;
       line-height: 0.8em;
}

/* Slide-specific styling */
#slide-inverse .footnote {
bottom: 12px;
left: 20px;
}
#slide-how .slides {
  font-size: 0.9em;
position: absolute;
top:  151px;
right: 140px;
}
#slide-how .slides h3 {
  margin-top: 0.2em;
}
#slide-how .slides .first, #slide-how .slides .second {
padding: 1px 20px;
height: 90px;
width: 120px;
       -moz-box-shadow: 0 0 10px #777;
       -webkit-box-shadow: 0 0 10px #777;
       box-shadow: 0 0 10px #777;
}
#slide-how .slides .first {
background: #fff;
position: absolute;
top: 20%;
left: 20%;
      z-index: 1;
}
#slide-how .slides .second {
position: relative;
background: #fff;
            z-index: 0;
}

/* Two-column layout */
.left-column {
color: #777;
width: 20%;
height: 92%;
float: left;
}
.left-column h2:last-of-type, .left-column h3:last-child {
color: #000;
}
.right-column {
width: 75%;
float: right;
       padding-top: 1em;
}
/* Large bullet point text */
.large-points
li
p {
  font-size: 26px;
}
</style>
  </head>
  <body>
    <textarea id="source">
name: inverse
layout: true
class: center, middle, inverse
---
# Spark Streaming in Action
An introduction to Spark Streaming through demonstration.
???
I'm Sean Glover.  I'm a co-organizer of the Toronto Scala & Typesafe Meetup group.  I do some OSS work in node.js *gasp*, but my favourite programming language is Scala by far.  I work as a Development Team Lead at a company called Ethoca.
---
layout: false
# Disclaimer
## I am not a Data Engineer, but I know a few.
???
* I'm not a Data Engineer, but I know a few.  
* Ever since I got into Scala in 2012 I've been more and more intrigued in data processing.  Once dominated by C, python, and Java an increasing number of organizations are recognizing the Scala is a wonderfully descriptive way to build your data platform from.  The emergence of data technologies based on Scala and Akka caught my eye and for the last year I've been doing a lot of self-study.  I've even had brief chances to work with some of these technologies on the job at InMoment and now Ethoca.

---
# Our itinerary..
(fade in each point as discussed, include tech logos Spark, Kafka, Confluent, Avro, Dispatch, GitHub)
* Spark
* Spark Demo
* Kafka & Confluent Stack
* Spark Streaming
* The Kafka Spark Streaming Plugin
* Message Delivery Semantics
* Spark Streaming Demo
* Cluster Infrastructure
???
* This is a talk about building a data streaming pipeline using Spark, Spark Streaming, and Kafka.
* I like practical talks myself, so I have a number of demos to show you today.  
* I'll first talk about Spark as a general purpose batch processing framework.  I have a small demo based on the StackOverflow.com data that I used in my Spark in Action talk at a meetup in early 2015.
* Next we'll talk about Kafka and the Confluent stack.
* I'll then describe Spark Streaming, the Kafka spark streaming plugin, and message delivery semantics.
* To put everything together I'll demo a Spark streaming application I've written that consumes the GitHub events API.
* Finally we'll talk about cluster infrastructure and the myriad of options available today.

Before I begin I'm curious
* Who has used any kind of stream processing or micro-batch framework before such as Storm, Samza, or something else?
* Can I get a show of hands of show has used Spark before?
* How about Kafka?

* Great, looks like I'm in good company!
---
.left-column[
## What is spark?
]
.right-column[
###Apache Spark is a general purpose computing engine for large-scale data processing

* Written in Scala

* Resilient Distributed Dataset (RDD) - Resilient through *lineage* (RDDs can always be reconstructed if nodes fail.)

* API support in **Scala**, Java, and Python

* Distributed in-memory datastore

* Interactive data exploration through Scala REPL

(update spark REPL screenshot for latest version)
<img src="spark-repl.png" alt="Spark REPL" style="width: 100%;"/>
]
???
* Spark was originally written by a team of researchers at Berkley.  The paper they published describes some of its core concepts really well.  I'll reference it a few times in this slide.
* Apache Spark is a general purpose computing engine for large-scale data processing
* Spark itself is written in Scala and runs on the JVM.


* One of the core components of Spark is the Reslient Distributed Dataset
  * Definition from the initial Spark paper from Berkeley:
  * "A resilient distributed dataset (RDD) is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost."

  * A cool thing about RDD's is how they respond to failure (i.e. a cluster of the node dies).  The solution is a technique they refer to as lineage.  Another definition from the Spark paper:
  * "The elements of an RDD need not exist in physical storage; instead, a handle to an RDD contains enough information to compute it starting from data in reliable storage. This means that they can always be reconstructed if nodes fail."
  * "RDDs achieve fault tolerance through a notion of lineage: if a partition of an RDD is lost, it has enough information about how it was derived from other RDDs to be able to rebuild just that partition"

  * RDD's are implemented as an iterable stream which makes it easy to plugin to existing language collections API's

* The Spark Scala API allows you to implement complex dataflows that use the standard Scala collections API functions like map, reduce, groupBy, filter, etc.

* After each step in the dataflow the intermediate data is stored across a distributed memory store

* You can launch Spark in a modified Scala REPL to experiment with your dataflow in the same way you experiment with your Scala application.
* Spark is the first system to allow an efficient, general purpose programming language to be used interactively to process large data sets on the cluster.

* (REPL image) There's an actual scala prompt at the bottom of this window, but I wanted to show the cool ASCII art!
---
class: large-points
.left-column[
## What is spark?
## Beyond MapReduce
]
.right-column[
###How is Spark any better than MapReduce?

  * No need to flush data between steps which makes **iterative jobs faster**

  * Cache large datasets in distributed memory to branch output operations and perform **interactive analysis**

  * An **intuitive API** that improves code readability and reduces typical MapReduce ceremony

```scala
val file = spark.textFile("hdfs://...")
val counts = file.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...") 
```
]
???
* Spark is different from standard MapReduce in a few ways.

* Iterative jobs. 
  * To implement these algorithms with Hadoop you're forced to dump to disk and then re-read your data between reduce steps.  Spark keeps this data in memory, which obviously has an enormous performance increase if your dataflow contains many steps.

* Interactive analysis.
  * Hadoop can be used for exploratory analysis on big datasets through SQL interfaces like Pig, but these operations are inherently slow because the data is usually still residing on disk.
  * Spark allows you to temporarily persist, or 'cache', a transformation or reduction to memory to use it later in your application.
  * This is useful because often times you want to use a single intermediate representation of your data and branch out to generate several different results

* The API for Scala allows you to write a lot less code than is typical on the Hadoop map and reduce framework.
  * MapReduce Data flows are usually more involved than a single map and reduce step
  * So Instead of trying to rework your algorithm to work with the MapReduce framework, Spark plugs into Scala's Collection API to make the implementation easier to write, read, and reason about.
---
# SampleApp.scala
```scala
/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
    val conf = new SparkConf().setAppName("Simple Application")
    val sc = new SparkContext(conf)
    val logData = sc.textFile(logFile, 2).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }
}
```
https://spark.apache.org/docs/latest/quick-start.html
???
* Sample Application straight from the Spark documentation quick start guide 
* What's in this example?
  * Instantiate a SparkContext
  * Load in the Spark README markdown file as sample data from the filesystem
  * `Suggest` that Spark `cache` the dataset in cluster memory so multiple operations can be performed on it.
  * Count the number of lines that contain a's and lines that contain b's
---
# Stack Exchange dataset 
<img src="stackexchange.png" alt="StackExchange" style="width:200px;"/>

*"All community-contributed content on Stack Exchange is licensed under the Creative Commons BY-SA 3.0 license. As part of our commitment to that, we release a quarterly dump of all user-contributed data (after carefully sanitizing it to protect user private data, of course)."*
https://archive.org/details/stackexchange

StackOverflow.com data:

(update this with latest, downloaded on laptop)
```bash
$ du stackoverflow.com*/*.xml --total --block-size=G
1G      stackoverflow.com-Badges/Badges.xml
8G      stackoverflow.com-Comments/Comments.xml
46G     stackoverflow.com-PostHistory/PostHistory.xml
1G      stackoverflow.com-PostLinks/PostLinks.xml
29G     stackoverflow.com-Posts/Posts.xml
1G      stackoverflow.com-Tags/Tags.xml
1G      stackoverflow.com-Users/Users.xml
7G      stackoverflow.com-Votes/Votes.xml
90G     total
``` 
???
* The examples I'm going to show you tonight are based StackOverflow data

* Every quarter, the Stack Exchange network releases all of its data for all of its sites under a Creative Commons license
* The dataset contains all Stack Exchange site posts, comments, badges, links, history, votes, users, and tags.
* It's scrubbed of all personally identifying information
* The dataset is available as a BitTorrent from the Internet Archive, it's about 21 GB compressed
* It's serialized as XML with one record per line

* For my demo I will be working exclusively with StackOverflow.com data which comes out to 90GB of uncompressed XML data
  * Of that I will only be working with the Posts.xml data which comes out to a paltry 29GB
  * (but it's just XML so take that with a grain of salt)
* Not a huge dataset, but for my limited computational resources it does the trick!
---
# Demo Stack Analysis app
???
* *Briefly show Stack Analysis app in IntelliJ.*
* *Run job from console with small subset of data that can return relatively fast (10 million rows?).  Turn off verbose debugging so there's not piles of loglines going by.. talk over a few of the important lines*

Run some commands from the console if there's time to demonstrate the REPL (grab last example from Spark in Action slide)
---
# Spark Streaming
<img src="streaming-dstream.png" />
http://spark.apache.org/docs/latest/streaming-programming-guide.html
<img src="streaming-dstream-window.png" />
http://spark.apache.org/docs/latest/streaming-programming-guide.html
???
* Spark Streaming is a micro-batch streaming addon for Apache Spark. A micro-batch is a small batch of events that are processed within a defined interval of time.  This differs from other "data streaming" technologies like Apache Storm or Samza which process events as soon as they're received.  The advantage of using a micro-batch approach with Spark is that it's possible to re-use the same business logic that was written for your batch processing Spark application.
* There are many different applications for streaming, but some common use cases are the following
  * Updating machine learning models as new information arrives.
    * Databricks has a good post about using k-means in a streaming application to this effect.  See my reference slide for the link.
  * Detecting anomalies, faults, performance problems, etc. and taking timely action.
  * Aggregating and processing data on arrival for downstream storage and analytics.
    * I would assume this is generally the most common use case and my demo is an example

* Akin to regular Spark batch processing, with Spark Streaming you have a Spark Streaming context.  This type is instantiated iwth the same parameters as a Spark Context, but in addition it also includes an option to set the interval in seconds that represents the time length of a micro-batch.  This could be a value from 1 second to minutes.  What to set this interval to depends on the latency requirements of your app.

* A core abstraction in Spark Streaming is the DStream, the discretized stream.  Internally, it's made up of a sequence of RDD's.  Each RDD represents an interval in time that as defined by the SparkStreaming Context.  
* You work with a DStream the same way you would with an RDD.  Many of the same operations are available such as map, flatMap, filter, reduce, etc.  There are also some stream-specific operations such as updateStateByKey and transform.  
* Transform is a useful operation when you want to perform an operation on the underlying RDD of this time slice.  This could be useful if you need to enrich the stream with data retrieved from another system, such as an analytical database.  You can make a batch call to your datasource by transforming over the entire RDD in one step instead of for each record at a time.

* There's also a set of operations that work over a sliding window of data.  These operations are known as windowed computations.  To illustrate the effect see the following figure from the Spark Streaming documentation.  Each windowed computation takes a user defined function and two parameters.

  * The window length, which represents how many intervals in the past you want to apply the computation to.
  * The slide interval, which represents how often to apply the computation.

* Both the window length and slide interval are defined in seconds and must be a multiple of the interval defined on the Spark Streaming Context.
---
# reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])
???
* As you would expect there are corresponding reduce and count operations using the sliding window, but the one I find most interesting is the reduceByKeyAndWindow overload that uses the inverse reducing function.  This makes the sliding window operation much more performant because it reuses computations from previous window intervals and calculates the delta.  It does this by having both a reduce function and an inverse reduce function which works by adding or substracting counts of counts of keys (or groups) as the window slides.
---
# Stream sources
* **Kafka**
* Flume
* Kinesis
* Twitter
* ZeroMQ
* MQTT
*and more..*
http://search.maven.org/#search|ga|1|g%3A%22org.apache.spark%22%20AND%20v%3A%221.5.0%22
???
* You can stream from a network socket or file, but there are many more advanced stream sources available:
  * **Kafka**
  * Flume
  * Kinesis
  * Twitter
  * ZeroMQ
  * MQTT
* There are more being developed every day that you can learn about on the spark-users group or just by checking out the latest mvnrepository under the org.apache.spark project.

---
# Kafka
???
Lots of organizations use Kafka to manage their event pipeline.
What is Kafka - fault tolerant, event log, producer-centric
Created by some devs at LinkedIn for user activity tracking on their site - lots of data
Devs created a company called Confluent that builds a stack around Kafka for interoperability, schema registration (assigning a schema to a topic), robust cross-platform serialization options (avro)
Show confluent architecture diagram
---
Message delivery semantics in Kafka
???
Talk about at most once, exactly once, and at least once semantics in Kafka and how they're achieved
---
Kafka as a Spark Streaming data source
???
Talk about new way of consuming Kafka streams (direct approach), contrast with old receiver appoach
 Direct experimental since 1.3
checkpointing - persisting state and data to a fault tolerant data store
---
Demo - GitHub /events api
???
Describe GitHub events api
  List caveats from notes
Describe tech stack (Dispatch, Kafka, Confluent (Schema Registry & Avro Serializer), Spark Stream, ... mongo/play?)
Run demo, show events using Confluent's Simple Consumer?
Show checkpoints
Show micro batches in Spark streaming console output
Show final web page?
---
References
Introducing streaming k-means in Spark 1.2 - https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html
???
* Dean Wampler is a Scala/Big Data evangelist at Typesafe.
* I don't think Dean knows who I am, but I'm a big fan.  If anyone in the crowd knows him personally give him a big kudos from his fan, Sean.  I'll buy'm a beer some day.
Dean has a great GitHub repo called the `spark-workshop` that helped me learn a lot about Spark's batch processing capabilities.  More recently he authored a whitepaper about using Spark Streaming with Typesafe's reactive stack, definitely worth checking out too.


















---
layout: false
# Why?
## Disclaimer: I am not a Data Engineer
.center[![Why?](why-data-science.png)]
.center[Why? Why? Why! - http://radar.oreilly.com/2013/04/why-why-why.html]
???
* Disclaimer:
* I'm not a data scientist or engineer
  * So why am I giving a presentation on Spark?
  * I'm a generalist.  I consider myself full stack.  Hadoop and now Spark have interested me for a long time and I figured that one way to help motivate me to follow through with that learning was present on it!
  * So here I am

* This comic is reinforcing the idea that before you invest too much time on a particular technology you really need to ask yourself what you're trying to get out of it.
  * In my case it's just an excuse to give me a better understanding of the space and I hope as a consequence you all get something out of it too.

---
.left-column[
## What is spark?
]
.right-column[
###Apache Spark is a general purpose computing engine for large-scale data processing

* Written in Scala

* Resilient Distributed Dataset (RDD) - Resilient through *lineage* (RDDs can always be reconstructed if nodes fail.)

* API support in **Scala**, Java, and Python

* Distributed in-memory datastore

* Interactive data exploration through Scala REPL

<img src="spark-repl.png" alt="Spark REPL" style="width: 100%;"/>
]
???
* Apache Spark is a general purpose computing engine for large-scale data processing
* Spark itself is written in Scala and runs on the JVM.

* One of the core components of Spark is the Reslient Distributed Dataset
  * Definition from the initial Spark paper from Berkeley:
  * "A resilient distributed dataset (RDD) is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost."

  * A cool thing about RDD's is how they respond to failure (i.e. a cluster of the node dies).  The solution is a technique they refer to as lineage.  Another definition from the Spark paper:
  * "The elements of an RDD need not exist in physical storage; instead, a handle to an RDD contains enough information to compute it starting from data in reliable storage. This means that they can always be reconstructed if nodes fail."
  * "RDDs achieve fault tolerance through a notion of lineage: if a partition of an RDD is lost, it has enough information about how it was derived from other RDDs to be able to rebuild just that partition"

  * RDD's are implemented as an iterable stream which makes it easy to plugin to existing language collections API's

* The Spark Scala API allows you to implement complex dataflows that use the standard Scala collections API functions like map, reduce, groupBy, filter, etc.

* After each step in the dataflow the intermediate data is stored across a distributed memory store

* You can launch Spark in a modified Scala REPL to experiment with your dataflow in the same way you experiment with your Scala application.
* Spark is the first system to allow an efficient, general purpose programming language to be used interactively to process large data sets on the cluster.

* (REPL image) There's an actual scala prompt at the bottom of this window, but I wanted to show the cool ASCII art!
* Also.. latest version is 1.3.0!
---
class: large-points
.left-column[
## What is spark?
## Isn't this MapReduce?
]
.right-column[
###How is Spark any better than MapReduce?

  * No need to flush data between steps which makes **iterative jobs faster**

  * Cache large datasets in distributed memory to branch output operations and perform **interactive analysis**

  * An **intuitive API** that improves code readability and reduces typical MapReduce ceremony

```scala
val file = spark.textFile("hdfs://...")
val counts = file.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...") 
```
]
???
* Spark is different from standard MapReduce in a few ways.

* Iterative jobs. 
  * Again, from the Berkley paper:
  * "Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter"
  * To implement these algorithms with Hadoop you're forced to dump to disk and then re-read your data between reduce steps.  Spark keeps this data in memory, which obviously has an enormous performance increase if your dataflow contains many steps.

* Interactive analysis.
  * The Berkley paper states
  * "Hadoop is often used to perform ad-hoc exploratory queries on big datasets, through SQL interfaces such as Pig and Hive. Ideally,
a user would be able to load a dataset of interest into memory across a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant latency (tens of seconds) because it runs as a separate MapReduce job and reads data from disk"
  * Spark allows you to temporarily persist, or 'cache', a transformation or reduction to memory to use it later in your application.
  * This is useful because often times you want to use a single intermediate representation of your data and branch out to generate several different results

* The API for Scala allows you to write a lot less code than is typical on the Hadoop map and reduce framework.
  * MapReduce Data flows are usually more involved than a single map and reduce step
  * So Instead of trying to rework your algorithm to work with the MapReduce framework, Spark plugs into Scala's Collection API to make the implementation easier to write, read, and reason about.
---
# Driver Application
* The SparkContext contains information used to run your driver app

  * Configured by env vars, spark-defaults.xml, CLI `spark-shell` & `spark-submit`

  * Singleton

  * Testing.  Change `master` by configuration.

* Data sources can be in memory collections, local filesystem, HDFS, HTTP
<div style="text-align:center">
<img src="hdfs-architecture.gif" alt="HDFS Architecture" style="width: 400px;"/>
</div>
???
* The SparkContext is the heart of the Spark implementation in your driver application.
  * It contains all the information required for your driver application to run.
  * You can configure the SparkContext with environment variables, the spark-defaults.xml, and through CLI with Spark helper scripts `spark-shell` & `spark-submit`
  * The SparkContext is a singleton.  So you can only have one SparkContext per JVM process.
  * Testing is fairly straightforward with Spark because it's easy to change your `master` Resource Manager via configuration.
    * For example in production you could be using a Hadoop YARN cluster, but for tests you can use a single core in the same process
    * Tests must be run in serial because of the one SparkContext per JVM rule

* Data sources the SparkContext supplies
  * The SparkContext Spark operates on a number of familiar data sources
  * Local filesystem, HDFS, HTTP
  * It's common to utilize HDFS for Spark application I/O
  * There's also a spark streaming project which allows you to stream data from a number of different sources.

---
# SampleApp.scala
```scala
/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
    val conf = new SparkConf().setAppName("Simple Application")
    val sc = new SparkContext(conf)
    val logData = sc.textFile(logFile, 2).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }
}
```
https://spark.apache.org/docs/latest/quick-start.html
???
* Sample Application straight from the Spark documentation quick start guide 
* What's in this example?
  * Instantiate a SparkContext
  * Load in the Spark README markdown file as sample data from the filesystem
  * `Suggest` that Spark `cache` the dataset in cluster memory so multiple operations can be performed on it.
  * Count the number of lines that contain a's and lines that contain b's
---
# Stack Exchange dataset 
<img src="stackexchange.png" alt="StackExchange" style="width:200px;"/>

*"All community-contributed content on Stack Exchange is licensed under the Creative Commons BY-SA 3.0 license. As part of our commitment to that, we release a quarterly dump of all user-contributed data (after carefully sanitizing it to protect user private data, of course)."*
http://blog.stackexchange.com/category/cc-wiki-dump/

StackOverflow.com data:

```bash
$ du stackoverflow.com*/*.xml --total --block-size=G
1G      stackoverflow.com-Badges/Badges.xml
8G      stackoverflow.com-Comments/Comments.xml
46G     stackoverflow.com-PostHistory/PostHistory.xml
1G      stackoverflow.com-PostLinks/PostLinks.xml
29G     stackoverflow.com-Posts/Posts.xml
1G      stackoverflow.com-Tags/Tags.xml
1G      stackoverflow.com-Users/Users.xml
7G      stackoverflow.com-Votes/Votes.xml
90G     total
``` 
???
* The examples I'm going to show you tonight are based StackOverflow data

* Every quarter, the Stack Exchange network releases all of its data for all of its sites under a Creative Commons license
* The dataset contains all Stack Exchange site posts, comments, badges, links, history, votes, users, and tags.
* It's scrubbed of all personally identifying information
* The dataset is available as a BitTorrent from the Internet Archive, it's about 21 GB compressed
* It's serialized as XML with one record per line

* For my demo I will be working exclusively with StackOverflow.com data which comes out to 90GB of uncompressed XML data
  * Of that I will only be working with the Posts.xml data which comes out to a paltry 29GB
  * (but it's just XML so take that with a grain of salt)
* Not a huge dataset, but for my limited computational resources it does the trick!
---
class: large-points
# ScalaTagCount demo in Standalone mode

## hulk

* Intel® Core™ i7-2600K CPU @ 3.40GHz × 8

* 16GB RAM

* ATA OCZ-VERTEX3 SSD

<img src="hulk.jpg" alt="Hulk" style="width:200px"/ >
???
* This is the first example I'll be running for you.
* It's a Spark driver application running on a single system I have called hulk.

* Although Spark has lots of cluster support, you can just as easily run it in on a single system utilizing all cores and memory you have available.
* You can run Spark Applications with a single library download and no other infrastructure to setup
* This makes it easy to develop and troubleshoot Spark Applications in isolation which makes it easier to learn.

* Count all distinct tags that are associated with StackOverflow questions that are tagged with `Scala`
* *Show StackAnalysis.scala and describe each step of data flow

* *Run StackAnalysis.scala* and show some of the output.. come back to results at end of presentation
---
class: large-points
# Spark on the cluster

## Spark supports lots of cluster connectivity options

* Spark Standalone <img src="spark-logo.png"  style="height: 30px;"/>
* Hadoop YARN <img src="hadoop-logo.png"  style="height: 30px;"/>
* Mesos <img src="mesos-logo.png" style="height: 30px;" />

<div style="text-align:center">
<img src="spark-cluster-overview.png" alt="Spark Cluster Overview" />
http://spark.apache.org/docs/latest/running-on-mesos.html
</div> 
???
* Spark was designed to run in a clustered environment

* You can compile or download specific binaries of Spark that run on each of the supported environments.

* With the Spark Standalone you can setup a Spark cluster without any dependency on any other Resource Mangers
* This is probably the best option for quick & dirty solutions, but if you want to build a cluster that other applications can run upon (like MapReduce), then it's a good idea to consider some of the other options

* Some other cluster options include YARN and Mesos.
* YARN is what encompasses the new Resource and Node Manager in the Hadoop 2.x ecosystem.  The purpose of YARN is to decouple these responsibilities from the daemon's of the traditional Hadoop ecosystem

* Apache Mesos is similar to YARN, but goes a step further in unifying a cluster into a single pool of resources that applications can use.
* Definition from Apache Mesos website:
* "Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively."
---
## Apache Mesos cluster demo

### StackOverflow.com posts in `.cache()` using `spark-shell`

### learning-spark project

* Google Cloud Compute (free tier)
* 1 master VM and 3 slave VM's, 4 total, `n1-standard-2` instance type
* 2 vCPU each, 8 vCPU total
* 7.5 GB RAM each, 30GB RAM total
* HDFS, 500 GB each, 2 TB total

<br />
```bash
$ gcloud compute instances list --project learning-spark
NAME                 ZONE          MACHINE_TYPE  INTERNAL_IP    EXTERNAL_IP     STATUS
development-5159-d3d us-central1-a n1-standard-2 10.217.7.180   146.148.xxx.xxx RUNNING
development-5159-b61 us-central1-a n1-standard-2 10.144.195.205 104.197.xxx.xxx RUNNING
development-5159-d9  us-central1-a n1-standard-2 10.173.40.36   104.197.xxx.xxx RUNNING
development-5159-5d7 us-central1-a n1-standard-2 10.8.67.247    104.154.xxx.xxx RUNNING
```
???
* This is my second demo

* I chose to use Google Cloud Compute service because they have an amazing deal of $300 or 60 days (whichever runs out first) to use nearly nearly any part of their platform
  * The main restrictions are that you can only have 8 vCPU total
  * The `n1-standard-2` image is the largest VM you can setup.
  * If you don't have a `hulk` laying around or want to play with a cluster setup then this is an excellent option.

* I setup an Apache Mesos cluster with the help of Mesosphere:
  * From their website:
  * "The Mesosphere is a new kind of operating system that organizes all of your machines, VMs, and cloud instances into a single pool of intelligently and dynamically shared resources. It runs on top of and enhances any modern version of Linux. ""
  * Mesosphere has excellent guides to setup Mesos clusters on various Infrastructure as as Service providers like Google Cloud Compute, Amazon Web Services, and Digital Ocean.
  * They even have scripts that allow you to automate the setup of these environments if you provide them with your provider access key
  * All I had to do to accomodate my setup was attach larger disks and update HDFS config to use them

* Show Google Cloud Computer console
* Show spark-shell console with cached dataset

* Show total count of scala questions

```
:paste

val q = StackAnalysis.questions(sc, "file:///home/seglo/source/learning-spark/data/stackexchange/stackoverflow.com-Posts/Posts1m.xml")

// scala questions
val scalaQs = qCache.filter { case (id, creationDate, tags) => tags.contains("scala") }.cache

scalaQs.count()
```

* Total scala questions by month
```
:paste
val sdfMonth = new java.text.SimpleDateFormat("yyyy-MM")

scalaQs.map { case (id, creationDate, tags) => (sdfMonth.format(creationDate), 1)} 
  .reduceByKey((a, b) => a + b) 
  .sortByKey(true)
  .collect 
  .foreach(println)
```

* ** show hulk output **
---
# References

### Spark
* [Spark Documentation](https://spark.apache.org/docs/latest/)
* [Spark: Cluster Computing with Working Sets](https://amplab.cs.berkeley.edu/wp-content/uploads/2011/06/Spark-Cluster-Computing-with-Working-Sets.pdf) - University of Berkely paper introducing Spark - Much of the introduction of this talk is paraphrased from this paper.
* **[`spark-workshop` by Dean Wampler](https://github.com/deanwampler/spark-workshop) - Spark impl. examples in a Typesafe activator project**
* [Cloudera: Introduction to Apache Spark developer training](http://www.slideshare.net/cloudera/spark-devwebinarslides-final?related=1)

### Clusters

* [Google Cloud Compute](https://cloud.google.com/compute/) 
* [Mesosphere in the Cloud](http://mesosphere.com/docs/getting-started/) - Tutorials to setup mesosphere on IaaS (Amazon, DigitalOcean, Google)
  * [Google Cloud Compute setup](http://mesosphere.com/docs/getting-started/cloud/google/)
  * [Running Spark on Mesos](http://spark.apache.org/docs/1.3.0/running-on-mesos.html) - Mesosphere docs are out-of-date, use this once mesosphere sets up cluster for you.
* [Spark atop Mesos on Google Cloud Platform](http://ceteri.blogspot.ca/2014/09/spark-atop-mesos-on-google-cloud.html)
* [Running Spark on Mesos](https://spark.apache.org/docs/1.3.0/running-on-mesos.html)
* [YARN Architecture](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)
* [HDFS Architecture](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)
* [`docker-spark` by SequenceIQ](https://github.com/sequenceiq/docker-spark) - If you want to play around with a YARN cluster config

### Data

* [Stack Exchange Creative Commons data](http://blog.stackexchange.com/category/cc-wiki-dump/)
---
# That's it!

For this presentation and sample code.

https://github.com/seglo/learning-spark/

## I'm a Software Engineer working at <a href="http://www.ethoca.com/" alt="ethoca"><img src="ethoca.jpg" style="height:40px" /></a> (we're hiring!)

* <a href=mailto:seglo@randonom.com>seglo@randonom.com</a>
* http://randonom.com/blog/
* https://github.com/seglo/
* https://twitter.com/randonom/
    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>